## 第一周 Introduction

你上传的两份讲义分别是：

- **01.1 Introduction**：介绍了 AI 和机器学习（ML）的基本概念、学习类型和课程结构。
- **01.2 Performance**：讲解了如何评估机器学习模型的性能，重点是分类任务中的准确率、召回率等指标。

------

### 01.1 Introduction – 第一章讲义讲了什么？

#### 本课程的核心问题：

本单元的核心目标是研究如何**从数据中自动推理（automate inference）**，即机器学习和人工智能的本质。课程强调 ML / AI 是一种以数据为中心的推理方法，并且具有实际导向性。

#### 课程结构（Unit Structure）：

1. **监督学习（Supervised Learning）**
   - 有标签的数据，如 `T = {(xᵢ, yᵢ)}`
   - 学习从输入 `x` 到标签 `y` 的映射关系 `C(x; θ)`
   - 示例：线性回归、图像分类（猫狗识别）、医学预测（阿尔兹海默症发展预测）
   - 本部分会讲：K 最近邻（k-NN）、回归（regression）、朴素贝叶斯（naive Bayes）、简单神经网络（simple neural networks）
2. **非监督学习（Unsupervised Learning）**
   - 没有标签，算法需自动发现数据的结构
   - 示例：对大量认知测验或脑部扫描结果进行聚类分析以探索疾病风险
   - 会讲到的算法包括：K 均值聚类（k-means）、层次聚类（hierarchical clustering）、竞争性聚类（competitive clustering）、高斯聚类（Gaussian clustering）、主成分方法（component methods）
3. **神经网络（Neural Networks）**
   - 一种复杂的输入输出映射，由多个“神经元”节点组成
   - 学习的过程就是调整权重 `w` 和偏置 `b`
   - 借鉴了大脑的结构，现在在图像识别、语言处理等领域取得了极大成功
4. **伦理问题（Ethics）**
   - 算法的决策过程不透明，可能放大数据中已有的偏见
   - 涉及数据所有权、回报分配问题
   - 举例：Wikipedia 数据是否应该为模型开发者带来收益？
5. **其他方法（Other Approaches）**
   - 包括基于代理（agent-based models）、搜索算法（search algorithms）、贝叶斯模型（Bayesian models）等
   - 特别提到 Hamiltonian Monte Carlo 正在引发贝叶斯建模的革命



### 01.2 Performance – 如何衡量分类模型好坏？

这部分讲解了如何**评估一个分类模型的性能**，是机器学习中的基础知识。

#### 核心概念：

1. **二分类任务常用指标**
   - **TP（True Positive）**: 预测为正且实际为正
   - **FP（False Positive）**: 预测为正但实际为负
   - **TN（True Negative）**: 预测为负且实际为负
   - **FN（False Negative）**: 预测为负但实际为正
2. **主要评价指标**
   - **精确率 Precision** = TP / (TP + FP)
      → 在预测为正的样本中，真正为正的比例
   - **召回率 Recall** = TP / (TP + FN)
      → 实际为正的样本中被正确识别出的比例
   - **F1 分数 F1 Score** = 调和平均 = 2 × (Precision × Recall) / (Precision + Recall)
3. **多分类问题（Multi-class Classification）**
   - **混淆矩阵（Confusion Matrix）**：每类的预测 vs 实际情况
   - **准确率（Accuracy）** = 正确预测数 / 总样本数
   - **宏平均（Macro-average）**：对每一类计算指标，然后平均
   - **微平均（Micro-average）**：将多类问题转换为正负二类问题后计算指标



### 拓展算法部分

拓展一下这四种经典的**监督学习（Supervised Learning）**方法：K 最近邻（k-NN）、回归（Regression）、朴素贝叶斯（Naive Bayes）、简单神经网络（Simple Neural Networks）

------

#### 1.1 K 最近邻算法 简述版（K-Nearest Neighbors, k-NN）

##### 核心思想

- 给定一个新的样本点，找出在训练集中距离它最近的 **K 个点**（邻居），看这些邻居中哪个类别占多数，就把新样本判为该类。
- 属于 **懒惰学习（lazy learning）**：不在训练阶段构建模型，预测时才计算。

##### 关键点

- **距离度量**：常用欧几里得距离（Euclidean Distance）
- **K 的选择**：K 太小容易过拟合，太大可能欠拟合
- **优点**：实现简单、无参数训练
- **缺点**：对高维数据和大数据量计算代价高，对噪声敏感



#### 1.2 K-最近邻算法的原理（k-NN）

##### 核心思想

> **一个样本的类别由它“邻居”中的大多数类别决定。**

------

##### 工作流程（原理 step by step）

假设我们要分类一个新样本 `x₀`，整个流程如下：

1. **计算距离（Distance Calculation）**
    计算 `x₀` 到训练集中所有样本的距离（最常用的是欧几里得距离）：

   $d(x0,xi)=∑j=1n(x0(j)−xi(j))2d(x₀, xᵢ) = \sqrt{\sum_{j=1}^{n}(x₀^{(j)} - xᵢ^{(j)})^2}$

2. **找出最近的 K 个邻居（K Nearest Neighbors）**
    将训练样本按照距离从小到大排序，选出距离最近的 K 个点。

3. **投票（Majority Voting）**
    查看这 K 个邻居中哪个类别（class）出现得最多，就将该类别作为 `x₀` 的预测结果。

------

##### 具体例子：

假设我们要预测某水果是“苹果”还是“橙子”，我们有如下数据集（根据颜色和重量）：

| 样本 | 颜色深度 | 重量(g) | 类别   |
| ---- | -------- | ------- | ------ |
| A    | 1.0      | 150     | 🍎 苹果 |
| B    | 0.8      | 170     | 🍊 橙子 |
| C    | 1.2      | 140     | 🍎 苹果 |
| D    | 0.9      | 160     | 🍊 橙子 |

新样本 E：（颜色 1.0，重量 155），我们计算它到 A~D 的距离，选 K=3，发现最近的 3 个邻居中有 2 个是苹果（A 和 C），1 个是橙子（D），所以预测 E 是苹果 🍎。



##### 可视化直观理解：

想象一个二维平面，点的颜色表示类别：

```
            ⬤（苹果）
      ○
               × 新样本
  ○       ○
            ⬤（橙子）
```

- 新样本的“邻居”是旁边的点们
- 看这些邻居是啥类型，就预测为哪一类



##### K 的选择很重要

- **K 太小（如 1）**：
  - 对噪声敏感，容易过拟合。
- **K 太大**：
  - 模型会变得太“模糊”，容易欠拟合。
- 通常通过交叉验证（cross-validation）选出最合适的 K。



##### 距离度量方式（常见）

1. **欧几里得距离（Euclidean Distance）**：

   $d(x,y)=∑i=1n(xi−yi)2d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$

2. **曼哈顿距离（Manhattan Distance）**：

   $d(x,y)=∑i=1n∣xi−yi∣d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$

3. **闵可夫斯基距离（Minkowski Distance）**（是前两者的推广）



##### KNN 的优缺点

| 优点                   | 缺点                               |
| ---------------------- | ---------------------------------- |
| 实现简单               | 预测时计算成本高（每次都得算距离） |
| 不需要训练过程         | 对高维数据表现差（维度灾难）       |
| 可用于分类也可用于回归 | 对异常值和噪声敏感                 |
| 效果直观、易理解       | 特征缩放对结果影响大（需标准化）   |



##### 注意事项

- 数据预处理非常重要：
  - 特征标准化（如 z-score、min-max scaling）防止某些特征对距离影响过大
- 通常结合 `cross-validation` 来确定 `K` 值
- 可通过 KD 树（K-D tree）或 Ball 树优化高维空间的搜索效率





#### 2.1 回归（Regression）

**核心思想：**

- 用于预测**连续数值型变量**（而非类别），目标是学习一个函数从输入 `x` 映射到输出 `y`。

**常见形式：**

###### 线性回归（Linear Regression）：

- 假设关系是线性的：

  $y=mx+c$

- 目标是最小化误差（如均方误差 MSE）来拟合参数 `m` 和 `c`

###### 多元线性回归（Multiple Linear Regression）：

- 输入变量 `x` 有多个维度：

  $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n$

###### 逻辑回归（Logistic Regression）：

- 实际上是分类算法，用于输出属于某一类的概率

  $P(y=1 | x) = \frac{1}{1 + e^{-(wx + b)}}$



#### 2.2 补充MSE(Mean Squared Error 均方误差)

MSE 是一种衡量“预测值和真实值之间误差”的方法，计算公式是：

$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

- $y_i$ 是真实值
- $\hat{y}_i $是模型预测值
- 差值平方后再求平均，可以衡量整体拟合效果好不好



#### 2.3 $R²$(R-squared）

R²（R-squared）是回归分析中非常重要的一个评估指标，被称为**判定系数**。



**一句话解释就是 R² 衡量的是模型解释目标变量变异性的能力**，也就是：

➤ **模型解释了多少比例的 y 的“变化”？**



R² 的经典定义是：

$R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}$ 

$$R^2(y, \hat{y})) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

其中：

| 符号                | 含义                                                         |
| ------------------- | ------------------------------------------------------------ |
| ${SS}_{\text{res}}$ | 残差平方和（Sum of Squares of Residuals） 也就是预测值和真实值之间的误差平方和 |
| ${SS}_{\text{tot}}$ | 总平方和（Total Sum of Squares） 也就是真实值和平均值之间的误差平方和 |
| $y_i$               | 第 i 个真实值                                                |
| $\hat{y}_i$         | 第 i 个预测值                                                |
| $\bar{y}$           | 所有真实值的平均值（目标均值）                               |

1. **SS_res** 衡量的是：

   > 我现在用模型预测，误差有多大？

2. **SS_tot** 衡量的是：

   > 如果我什么都不预测，只是拿 “平均值” 当预测，误差有多大？

3. 所以 R² 本质上是：

   $R^2 = 1 - \frac{\text{你现在的模型的误差}}{\text{啥都不干直接用平均值的误差}}$



| R² 值      | 意义                                         |
| ---------- | -------------------------------------------- |
| R² = 1     | 模型完美预测，误差为 0                       |
| R² = 0     | 模型不如直接用均值预测                       |
| R² < 0     | 模型比“用平均值”还差！😬                      |
| 0 < R² < 1 | 模型解释了一定比例的 y 的变化，越接近 1 越好 |



#### 2.4 交叉验证（Cross-Validation, 简称 CV）

**交叉验证（Cross-Validation, 简称 CV）** 是模型训练中最常用、最可靠的评估技术之一，掌握它能让你训练出更**稳健**、更**泛化**的模型.



**问:  什么是交叉验证（Cross-Validation）？**

交叉验证是一种评估模型泛化能力的方法，核心思想是：把数据切分成多个“训练集+验证集”，多次训练和测试，最后平均结果



**问: 为什么不用一次简单的 train/test 划分就好？**

一次划分太“偶然”了，可能刚好：

- 测试集全是容易预测的 → 你以为模型很好，其实没学到本质
- 测试集难度太大 → 你以为模型很差，其实换个数据就好了

所以我们不只分一次，而是 **多次分，交替使用每一部分数据来训练和验证**，最后取平均，更公平、更稳定。



**最常用的交叉验证方法：K折交叉验证（K-Fold Cross-Validation）**

**问: 如何做?**

假设你设置 K=5，整个流程是这样的：

1. 把数据分成 5 个“相等大小的子集”
2. **每次轮流用其中一个作为验证集**，其他 4 个用作训练集
3. 共训练和评估 5 次
4. **对这 5 次的得分（如 MSE、R²、准确率）取平均** → 最终评分



**cross_val_score 常见参数**

| 参数        | 作用                                                         |
| ----------- | ------------------------------------------------------------ |
| `estimator` | 要使用的模型，比如 `LinearRegression()`                      |
| `X`、`y`    | 数据                                                         |
| `cv=5`      | 使用几折交叉验证                                             |
| `scoring`   | 使用什么指标评估，常见：`'r2'`、`'neg_mean_squared_error'`、`'accuracy'` |



**总结 : 交叉验证 = 多次训练+验证 → 取平均 → 更稳健的评估方法**

#### 2.5 通过交叉验证（cross-validation）来选择 KNN 模型中的最优超参数 `k`（邻居数）

你要做的就是：

> 在训练集上，使用交叉验证测试不同的 `k`（从 1 到 30），看看哪个 `k` 的表现最好，然后选它作为最终模型的参数。



**为什么要做这一步？**

- `k` 是 KNN 模型的超参数，它决定了模型的“邻居数量”
- `k` 太小 → 容易被噪声影响，模型不稳定（过拟合）
- `k` 太大 → 模型太“平均”，分不清边界（欠拟合）

**所以：我们需要找到一个“刚刚好”的 `k`**



**为什么用交叉验证？**

> 因为我们不能只靠一次 train/test 划分来判断 `k` 的好坏，那样可能是巧合。

交叉验证能让我们：

- 多次测试模型在不同验证集上的表现
- 得到更稳健的平均结果





#### 3. 朴素贝叶斯分类器（Naive Bayes）

##### 核心思想：

- 基于**贝叶斯定理（Bayes' Theorem）\**构建的分类器，假设各个特征之间\**条件独立（conditional independence）**

  $P(y∣x1,x2,...,xn)∝P(y)∏i=1nP(xi∣y)P(y|x_1, x_2, ..., x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)$



##### 贝叶斯定理是什么？

对于一个样本 xxx，希望预测它的类别 yyy 是哪一个：$P(y \mid x) = \frac{P(x \mid y) P(y)}{P(x)}$

其中：

- $P(y \mid x)$：在观察到特征 $x$ 后，它属于类别 $y$ 的**后验概率**
- $P(x \mid y)$：在类别 $y$ 下，观察到特征 $x$ 的概率（**似然**）
- $P(y)$：类别 $y$ 的先验概率
- $P(x)$：观察到样本 $x$ 的概率（不依赖类别）

我们要找的是：$\hat{y} = \arg\max_y P(y \mid x)$



##### 什么是“朴素”的地方？

因为 $x$ 是一个多维特征（有多个变量），我们做了一个**朴素假设**：

> **各个特征是条件独立的**

也就是：

$P(x_1, x_2, ..., x_n \mid y) = \prod_{i=1}^n P(x_i \mid y)$

所以朴素贝叶斯变成了：

$\hat{y} = \arg\max_y P(y) \prod_{i=1}^n P(x_i \mid y)$



**举例说明：**

- 假设你想预测一个邮件是否为垃圾邮件（spam），你会根据是否包含某些词（如“free”、“win”）来估算其概率。
- 如果训练集中出现“free”的邮件有 90% 是垃圾邮件，则 `P(spam|free)` 会很高。

**优点：**

- 速度快、占用内存小、对高维数据效果好（如文本分类）
- **尤其适合文本数据**（如情感分析、垃圾邮件识别）

**缺点：**

- 强假设“特征之间独立”在现实中通常不成立



------

#### 4. 简单神经网络（Simple Neural Networks）

**核心思想：**

- 模拟人脑神经元的结构，通过网络层级连接输入与输出，学习复杂的非线性映射关系。

**基本结构：**

- **输入层（Input Layer）**：特征向量
- **隐藏层（Hidden Layers）**：由多个神经元（节点）组成，进行加权和 + 激活函数（如 ReLU、sigmoid）
- **输出层（Output Layer）**：分类概率或数值预测

**单个神经元的计算：**

$z=∑wixi+b,激活后a=σ(z)z = \sum w_i x_i + b,\quad \text{激活后} \quad a = \sigma(z)$

其中 $σ\sigma$ 是激活函数（如 sigmoid, ReLU）

**训练过程：**

- 使用反向传播（backpropagation）和梯度下降（gradient descent）不断优化网络中的权重 `w` 和偏置 `b`

**优点：**

- 可逼近任意复杂函数（Universal Approximation Theorem）
- 支持多分类、多任务、图像处理、语言建模等多种应用

**缺点：**

- 可解释性差，需要大量数据训练
- 易过拟合，尤其在参数过多时



##### 4.2 单层感知器 (SLP) 版本



> **单层感知器（SLP, Single-Layer Perceptron）**

它是神经网络的“鼻祖”模型，也是理解深度学习的起点。


 **一句话总结 SLP：**

> **SLP 是一个只能处理线性可分问题的二分类模型**，它的核心是：
>  输入 → 加权求和 → 激活函数（通常是阶跃函数）→ 输出 0 或 1



**🧱 单层感知器结构**

基本形式：

$\hat{y} = \text{step}(w \cdot x + b)$

- $x$：输入向量（样本特征）
- $w$：权重向量（每个特征一个）
- $b$：偏置（bias）
- $\text{step}$：阶跃函数（激活函数）

输出只可能是 0 或 1（用于二分类）

------

**🔁 SLP 的训练过程（感知器算法）**

**步骤如下：**

对于每个训练样本 $(x_i, y_i)$：

1. **线性组合**：

   $z = w \cdot x_i + b$

2. **激活函数**（阶跃函数）：

   $\hat{y}_i = \text{step}(z) =  \begin{cases} 1, & z \ge 0 \\ 0, & z < 0 \end{cases}$

3. **计算误差**：

   $\text{error} = y_i - \hat{y}_i$

4. **更新权重和偏置**（感知器学习规则）：

   $w = w + \alpha \cdot \text{error} \cdot x_i$

   $b = b + \alpha \cdot \text{error}$

其中$\alpha $是学习率（learning rate）





**SLP 的优点**

| 优点       | 描述                       |
| ---------- | -------------------------- |
| 简单       | 非常容易理解和实现         |
| 快速       | 每次更新只需要一次计算     |
| 可解释性强 | 权重表示特征影响方向和程度 |

------

 **SLP 的局限**

| 局限                 | 描述                                 |
| -------------------- | ------------------------------------ |
| 只能处理线性可分问题 | 像 XOR 这种数据 SLP 无法正确分类     |
| 不能学习非线性边界   | 需要加隐藏层（→ MLP）才能扩展        |
| 二分类               | 默认只支持两类（扩展成多类比较复杂） |

------

 **SLP vs MLP（对比）**

| 特点           | SLP                | MLP                         |
| -------------- | ------------------ | --------------------------- |
| 层数           | 只有一层（输出层） | 至少有一个隐藏层            |
| 表达能力       | 线性               | 非线性                      |
| 是否能解决 XOR | ❌ 不行             | ✅ 可以                      |
| 激活函数       | 通常用 step        | 可用 ReLU, sigmoid, tanh 等 |

------

总结一句话：

> 单层感知器（SLP）是神经网络的最基础版本，只包含一层线性单元和一个激活函数，它**只能解决线性可分问题**，是理解深度学习模型的重要第一步。

------

你现在可以：

- 用它跑一个小数据集（比如乳腺癌）
- 和 KNN、Logistic Regression 做准确率对比
- 可视化它的分类边界（如果你愿意，我可以帮你画图！）

要继续探索感知器 → 多层感知器（MLP）吗？😄





## 无监督学习

### K-均值聚类（K-Means Clustering）

K-均值是一种将数据划分为 K 个“簇（cluster）”的算法，目标是让同一簇中的数据彼此更接近，不同簇之间差异更大。



**场景示例：**

比如你有很多用户购买数据，但没有标签。你想把用户自动分组（比如：高消费、中等消费、低消费）——这时就可以用 K-Means 聚类！



**K-Means 的核心思想**

K-Means 会尝试找到 K 个**“聚类中心（centroids）”**，并把每个样本归到**离自己最近的中心**：

$\text{目标：最小化簇内样本到簇中心的平方距离总和}$



**算法流程（K-Means Step-by-Step）**

假设你想分成 K 个簇，数据是 $X = \{x_1, x_2, ..., x_n\}$



**步骤如下：**

------

**第一步：初始化**

随机选 K 个样本作为初始“簇中心”（centroids）：

$\mu_1, \mu_2, ..., \mu_K$



**第二步：分配（Assignment）**

对于每个数据点 $x_i$，计算它到每个中心的距离，分配到最近的那个簇：

$c_i = \arg\min_k \|x_i - \mu_k\|^2$



**第三步：更新（Update）**

对每个簇，重新计算它的中心（取所有分配到它的点的均值）：

$\mu_k = \frac{1}{N_k} \sum_{i \in C_k} x_ii$



**第四步：重复**

重复执行 **分配 → 更新**，直到簇不再变化，或者变化很小（收敛）。



**K-Means 的目标函数（优化的东西）**

K-Means 最小化的是 **簇内平方和误差（Within-Cluster Sum of Squares, WCSS）**：

$\sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2$



**K-Means 的优缺点**

| 优点               | 缺点                                       |
| ------------------ | ------------------------------------------ |
| 简单易实现，速度快 | 只能发现凸型簇，不适合复杂分布             |
| 适合大数据         | 初始点选择对结果敏感（解决方法：KMeans++） |
| 聚类结果易解释     | 需要预先设定 K 值                          |



**总结一句话：**

> **K-Means 是一种高效的聚类算法，通过反复“分配+更新”，把数据划分成 K 个紧密的簇，目标是让每个簇内部更紧密，簇之间更分离。**



### 高斯混合模型（GMM, Gaussian Mixture Models）

它是一种 **概率模型**，用于表示具有多个高斯分布组成的复杂数据分布,一种比 K-Means 更灵活、更强大的聚类方法.

一句话总结就是GMM 是多个高斯分布（正态分布）的加权组合，每个分布对应一个“簇”。



**为什么要用 GMM？**

K-Means 假设每个簇是**一个球形**、大小相同、边界清晰的区域，但现实中的数据往往：

- 不是球形
- 各簇大小/方向不同
- 有重叠区域（模糊边界）

GMM 就能处理这些复杂情况！



**GMM 与 K-Means 的对比：**

| 特点     | K-Means                  | GMM                            |
| -------- | ------------------------ | ------------------------------ |
| 分配方式 | 硬分配（每点属于一个簇） | 软分配（每点属于所有簇的概率） |
| 形状假设 | 球形簇（距离最小）       | 任意椭圆形（基于高斯概率）     |
| 簇模型   | 中心点（Centroid）       | 高斯分布（均值 + 协方差）      |
| 优势     | 快速、简单               | 更精确、更灵活                 |



**GMM 的数学形式**

GMM 假设一个样本 $x $来自 $K$ 个高斯分布之一：

$p(x) = \sum_{k=1}^K \pi_k \cdot \mathcal{N}(x \mid \mu_k, \Sigma_k)$

其中：

- $pi_kπk$：第 $k$ 个高斯分布的权重（概率和为 1）
- ${N}(x \mid \mu_k, \Sigma_k)$：第 $k$ 个高斯分布
  - $\mu_kμk$：均值
  - $\Sigma_k$：协方差矩阵（可以控制形状/方向）



**GMM 的训练过程：EM算法**

GMM 通过一种叫做 **期望最大化（EM）** 的算法进行学习：

| 步骤 | 含义                                                       |
| ---- | ---------------------------------------------------------- |
| E 步 | 计算每个点属于每个高斯簇的**概率**                         |
| M 步 | 根据这些概率，**更新每个高斯的参数**（均值、协方差、权重） |

重复 E + M，直到收敛。