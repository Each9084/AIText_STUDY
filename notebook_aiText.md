## 第一周 Introduction

你上传的两份讲义分别是：

- **01.1 Introduction**：介绍了 AI 和机器学习（ML）的基本概念、学习类型和课程结构。
- **01.2 Performance**：讲解了如何评估机器学习模型的性能，重点是分类任务中的准确率、召回率等指标。

------

### 01.1 Introduction – 第一章讲义讲了什么？

#### 本课程的核心问题：

本单元的核心目标是研究如何**从数据中自动推理（automate inference）**，即机器学习和人工智能的本质。课程强调 ML / AI 是一种以数据为中心的推理方法，并且具有实际导向性。

#### 课程结构（Unit Structure）：

1. **监督学习（Supervised Learning）**
   - 有标签的数据，如 `T = {(xᵢ, yᵢ)}`
   - 学习从输入 `x` 到标签 `y` 的映射关系 `C(x; θ)`
   - 示例：线性回归、图像分类（猫狗识别）、医学预测（阿尔兹海默症发展预测）
   - 本部分会讲：K 最近邻（k-NN）、回归（regression）、朴素贝叶斯（naive Bayes）、简单神经网络（simple neural networks）
2. **非监督学习（Unsupervised Learning）**
   - 没有标签，算法需自动发现数据的结构
   - 示例：对大量认知测验或脑部扫描结果进行聚类分析以探索疾病风险
   - 会讲到的算法包括：K 均值聚类（k-means）、层次聚类（hierarchical clustering）、竞争性聚类（competitive clustering）、高斯聚类（Gaussian clustering）、主成分方法（component methods）
3. **神经网络（Neural Networks）**
   - 一种复杂的输入输出映射，由多个“神经元”节点组成
   - 学习的过程就是调整权重 `w` 和偏置 `b`
   - 借鉴了大脑的结构，现在在图像识别、语言处理等领域取得了极大成功
4. **伦理问题（Ethics）**
   - 算法的决策过程不透明，可能放大数据中已有的偏见
   - 涉及数据所有权、回报分配问题
   - 举例：Wikipedia 数据是否应该为模型开发者带来收益？
5. **其他方法（Other Approaches）**
   - 包括基于代理（agent-based models）、搜索算法（search algorithms）、贝叶斯模型（Bayesian models）等
   - 特别提到 Hamiltonian Monte Carlo 正在引发贝叶斯建模的革命



### 01.2 Performance – 如何衡量分类模型好坏？

这部分讲解了如何**评估一个分类模型的性能**，是机器学习中的基础知识。

#### 核心概念：

1. **二分类任务常用指标**
   - **TP（True Positive）**: 预测为正且实际为正
   - **FP（False Positive）**: 预测为正但实际为负
   - **TN（True Negative）**: 预测为负且实际为负
   - **FN（False Negative）**: 预测为负但实际为正
2. **主要评价指标**
   - **精确率 Precision** = TP / (TP + FP)
      → 在预测为正的样本中，真正为正的比例
   - **召回率 Recall** = TP / (TP + FN)
      → 实际为正的样本中被正确识别出的比例
   - **F1 分数 F1 Score** = 调和平均 = 2 × (Precision × Recall) / (Precision + Recall)
3. **多分类问题（Multi-class Classification）**
   - **混淆矩阵（Confusion Matrix）**：每类的预测 vs 实际情况
   - **准确率（Accuracy）** = 正确预测数 / 总样本数
   - **宏平均（Macro-average）**：对每一类计算指标，然后平均
   - **微平均（Micro-average）**：将多类问题转换为正负二类问题后计算指标



### 拓展算法部分

拓展一下这四种经典的**监督学习（Supervised Learning）**方法：K 最近邻（k-NN）、回归（Regression）、朴素贝叶斯（Naive Bayes）、简单神经网络（Simple Neural Networks）

------

#### 1.1 K 最近邻算法 简述版（K-Nearest Neighbors, k-NN）

##### 核心思想

- 给定一个新的样本点，找出在训练集中距离它最近的 **K 个点**（邻居），看这些邻居中哪个类别占多数，就把新样本判为该类。
- 属于 **懒惰学习（lazy learning）**：不在训练阶段构建模型，预测时才计算。

##### 关键点

- **距离度量**：常用欧几里得距离（Euclidean Distance）
- **K 的选择**：K 太小容易过拟合，太大可能欠拟合
- **优点**：实现简单、无参数训练
- **缺点**：对高维数据和大数据量计算代价高，对噪声敏感



#### 1.2 K-最近邻算法的原理（k-NN）

##### 核心思想

> **一个样本的类别由它“邻居”中的大多数类别决定。**

------

##### 工作流程（原理 step by step）

假设我们要分类一个新样本 `x₀`，整个流程如下：

1. **计算距离（Distance Calculation）**
    计算 `x₀` 到训练集中所有样本的距离（最常用的是欧几里得距离）：

   $d(x0,xi)=∑j=1n(x0(j)−xi(j))2d(x₀, xᵢ) = \sqrt{\sum_{j=1}^{n}(x₀^{(j)} - xᵢ^{(j)})^2}$

2. **找出最近的 K 个邻居（K Nearest Neighbors）**
    将训练样本按照距离从小到大排序，选出距离最近的 K 个点。

3. **投票（Majority Voting）**
    查看这 K 个邻居中哪个类别（class）出现得最多，就将该类别作为 `x₀` 的预测结果。

------

##### 具体例子：

假设我们要预测某水果是“苹果”还是“橙子”，我们有如下数据集（根据颜色和重量）：

| 样本 | 颜色深度 | 重量(g) | 类别   |
| ---- | -------- | ------- | ------ |
| A    | 1.0      | 150     | 🍎 苹果 |
| B    | 0.8      | 170     | 🍊 橙子 |
| C    | 1.2      | 140     | 🍎 苹果 |
| D    | 0.9      | 160     | 🍊 橙子 |

新样本 E：（颜色 1.0，重量 155），我们计算它到 A~D 的距离，选 K=3，发现最近的 3 个邻居中有 2 个是苹果（A 和 C），1 个是橙子（D），所以预测 E 是苹果 🍎。



##### 可视化直观理解：

想象一个二维平面，点的颜色表示类别：

```
            ⬤（苹果）
      ○
               × 新样本
  ○       ○
            ⬤（橙子）
```

- 新样本的“邻居”是旁边的点们
- 看这些邻居是啥类型，就预测为哪一类



##### K 的选择很重要

- **K 太小（如 1）**：
  - 对噪声敏感，容易过拟合。
- **K 太大**：
  - 模型会变得太“模糊”，容易欠拟合。
- 通常通过交叉验证（cross-validation）选出最合适的 K。



##### 距离度量方式（常见）

1. **欧几里得距离（Euclidean Distance）**：

   $d(x,y)=∑i=1n(xi−yi)2d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$

2. **曼哈顿距离（Manhattan Distance）**：

   $d(x,y)=∑i=1n∣xi−yi∣d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$

3. **闵可夫斯基距离（Minkowski Distance）**（是前两者的推广）



##### KNN 的优缺点

| 优点                   | 缺点                               |
| ---------------------- | ---------------------------------- |
| 实现简单               | 预测时计算成本高（每次都得算距离） |
| 不需要训练过程         | 对高维数据表现差（维度灾难）       |
| 可用于分类也可用于回归 | 对异常值和噪声敏感                 |
| 效果直观、易理解       | 特征缩放对结果影响大（需标准化）   |



##### 注意事项

- 数据预处理非常重要：
  - 特征标准化（如 z-score、min-max scaling）防止某些特征对距离影响过大
- 通常结合 `cross-validation` 来确定 `K` 值
- 可通过 KD 树（K-D tree）或 Ball 树优化高维空间的搜索效率





#### 2. 回归（Regression）

##### 核心思想：

- 用于预测**连续数值型变量**（而非类别），目标是学习一个函数从输入 `x` 映射到输出 `y`。

##### 常见形式：

###### 线性回归（Linear Regression）：

- 假设关系是线性的：

  y=mx+cy = mx + c

- 目标是最小化误差（如均方误差 MSE）来拟合参数 `m` 和 `c`

###### 多元线性回归（Multiple Linear Regression）：

- 输入变量 `x` 有多个维度：

  y=β0+β1x1+β2x2+⋯+βnxny = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n

###### 逻辑回归（Logistic Regression）：

- 实际上是分类算法，用于输出属于某一类的概率

  P(y=1∣x)=11+e−(wx+b)P(y=1 | x) = \frac{1}{1 + e^{-(wx + b)}}

------

#### 3. 朴素贝叶斯分类器（Naive Bayes）

##### 核心思想：

- 基于**贝叶斯定理（Bayes' Theorem）\**构建的分类器，假设各个特征之间\**条件独立（conditional independence）**

  $P(y∣x1,x2,...,xn)∝P(y)∏i=1nP(xi∣y)P(y|x_1, x_2, ..., x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)$

##### 举例说明：

- 假设你想预测一个邮件是否为垃圾邮件（spam），你会根据是否包含某些词（如“free”、“win”）来估算其概率。
- 如果训练集中出现“free”的邮件有 90% 是垃圾邮件，则 `P(spam|free)` 会很高。

##### 优点：

- 速度快、占用内存小、对高维数据效果好（如文本分类）
- **尤其适合文本数据**（如情感分析、垃圾邮件识别）

##### 缺点：

- 强假设“特征之间独立”在现实中通常不成立

------

#### 4️⃣ 简单神经网络（Simple Neural Networks）

##### 📌 核心思想：

- 模拟人脑神经元的结构，通过网络层级连接输入与输出，学习复杂的非线性映射关系。

##### 🧱 基本结构：

- **输入层（Input Layer）**：特征向量
- **隐藏层（Hidden Layers）**：由多个神经元（节点）组成，进行加权和 + 激活函数（如 ReLU、sigmoid）
- **输出层（Output Layer）**：分类概率或数值预测

##### ⚙️ 单个神经元的计算：

$z=∑wixi+b,激活后a=σ(z)z = \sum w_i x_i + b,\quad \text{激活后} \quad a = \sigma(z)$

其中 $σ\sigma$ 是激活函数（如 sigmoid, ReLU）

##### 🧠 训练过程：

- 使用反向传播（backpropagation）和梯度下降（gradient descent）不断优化网络中的权重 `w` 和偏置 `b`

##### ✅ 优点：

- 可逼近任意复杂函数（Universal Approximation Theorem）
- 支持多分类、多任务、图像处理、语言建模等多种应用

##### ❌ 缺点：

- 可解释性差，需要大量数据训练
- 易过拟合，尤其在参数过多时