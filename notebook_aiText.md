## 第一周 Introduction

你上传的两份讲义分别是：

- **01.1 Introduction**：介绍了 AI 和机器学习（ML）的基本概念、学习类型和课程结构。
- **01.2 Performance**：讲解了如何评估机器学习模型的性能，重点是分类任务中的准确率、召回率等指标。

------

### 01.1 Introduction – 第一章讲义讲了什么？

#### 本课程的核心问题：

本单元的核心目标是研究如何**从数据中自动推理（automate inference）**，即机器学习和人工智能的本质。课程强调 ML / AI 是一种以数据为中心的推理方法，并且具有实际导向性。

#### 课程结构（Unit Structure）：

1. **监督学习（Supervised Learning）**
   - 有标签的数据，如 `T = {(xᵢ, yᵢ)}`
   - 学习从输入 `x` 到标签 `y` 的映射关系 `C(x; θ)`
   - 示例：线性回归、图像分类（猫狗识别）、医学预测（阿尔兹海默症发展预测）
   - 本部分会讲：K 最近邻（k-NN）、回归（regression）、朴素贝叶斯（naive Bayes）、简单神经网络（simple neural networks）
2. **非监督学习（Unsupervised Learning）**
   - 没有标签，算法需自动发现数据的结构
   - 示例：对大量认知测验或脑部扫描结果进行聚类分析以探索疾病风险
   - 会讲到的算法包括：K 均值聚类（k-means）、层次聚类（hierarchical clustering）、竞争性聚类（competitive clustering）、高斯聚类（Gaussian clustering）、主成分方法（component methods）
3. **神经网络（Neural Networks）**
   - 一种复杂的输入输出映射，由多个“神经元”节点组成
   - 学习的过程就是调整权重 `w` 和偏置 `b`
   - 借鉴了大脑的结构，现在在图像识别、语言处理等领域取得了极大成功
4. **伦理问题（Ethics）**
   - 算法的决策过程不透明，可能放大数据中已有的偏见
   - 涉及数据所有权、回报分配问题
   - 举例：Wikipedia 数据是否应该为模型开发者带来收益？
5. **其他方法（Other Approaches）**
   - 包括基于代理（agent-based models）、搜索算法（search algorithms）、贝叶斯模型（Bayesian models）等
   - 特别提到 Hamiltonian Monte Carlo 正在引发贝叶斯建模的革命



### 01.2 Performance – 如何衡量分类模型好坏？

这部分讲解了如何**评估一个分类模型的性能**，是机器学习中的基础知识。

#### 核心概念：

1. **二分类任务常用指标**
   - **TP（True Positive）**: 预测为正且实际为正
   - **FP（False Positive）**: 预测为正但实际为负
   - **TN（True Negative）**: 预测为负且实际为负
   - **FN（False Negative）**: 预测为负但实际为正
2. **主要评价指标**
   - **精确率 Precision** = TP / (TP + FP)
      → 在预测为正的样本中，真正为正的比例
   - **召回率 Recall** = TP / (TP + FN)
      → 实际为正的样本中被正确识别出的比例
   - **F1 分数 F1 Score** = 调和平均 = 2 × (Precision × Recall) / (Precision + Recall)
3. **多分类问题（Multi-class Classification）**
   - **混淆矩阵（Confusion Matrix）**：每类的预测 vs 实际情况
   - **准确率（Accuracy）** = 正确预测数 / 总样本数
   - **宏平均（Macro-average）**：对每一类计算指标，然后平均
   - **微平均（Micro-average）**：将多类问题转换为正负二类问题后计算指标



### 拓展算法部分

拓展一下这四种经典的**监督学习（Supervised Learning）**方法：K 最近邻（k-NN）、回归（Regression）、朴素贝叶斯（Naive Bayes）、简单神经网络（Simple Neural Networks）

------

#### 1.1 K 最近邻算法 简述版（K-Nearest Neighbors, k-NN）

##### 核心思想

- 给定一个新的样本点，找出在训练集中距离它最近的 **K 个点**（邻居），看这些邻居中哪个类别占多数，就把新样本判为该类。
- 属于 **懒惰学习（lazy learning）**：不在训练阶段构建模型，预测时才计算。

##### 关键点

- **距离度量**：常用欧几里得距离（Euclidean Distance）
- **K 的选择**：K 太小容易过拟合，太大可能欠拟合
- **优点**：实现简单、无参数训练
- **缺点**：对高维数据和大数据量计算代价高，对噪声敏感



#### 1.2 K-最近邻算法的原理（k-NN）

##### 核心思想

> **一个样本的类别由它“邻居”中的大多数类别决定。**

------

##### 工作流程（原理 step by step）

假设我们要分类一个新样本 `x₀`，整个流程如下：

1. **计算距离（Distance Calculation）**
    计算 `x₀` 到训练集中所有样本的距离（最常用的是欧几里得距离）：

   $d(x0,xi)=∑j=1n(x0(j)−xi(j))2d(x₀, xᵢ) = \sqrt{\sum_{j=1}^{n}(x₀^{(j)} - xᵢ^{(j)})^2}$

2. **找出最近的 K 个邻居（K Nearest Neighbors）**
    将训练样本按照距离从小到大排序，选出距离最近的 K 个点。

3. **投票（Majority Voting）**
    查看这 K 个邻居中哪个类别（class）出现得最多，就将该类别作为 `x₀` 的预测结果。

------

##### 具体例子：

假设我们要预测某水果是“苹果”还是“橙子”，我们有如下数据集（根据颜色和重量）：

| 样本 | 颜色深度 | 重量(g) | 类别   |
| ---- | -------- | ------- | ------ |
| A    | 1.0      | 150     | 🍎 苹果 |
| B    | 0.8      | 170     | 🍊 橙子 |
| C    | 1.2      | 140     | 🍎 苹果 |
| D    | 0.9      | 160     | 🍊 橙子 |

新样本 E：（颜色 1.0，重量 155），我们计算它到 A~D 的距离，选 K=3，发现最近的 3 个邻居中有 2 个是苹果（A 和 C），1 个是橙子（D），所以预测 E 是苹果 🍎。



##### 可视化直观理解：

想象一个二维平面，点的颜色表示类别：

```
            ⬤（苹果）
      ○
               × 新样本
  ○       ○
            ⬤（橙子）
```

- 新样本的“邻居”是旁边的点们
- 看这些邻居是啥类型，就预测为哪一类



##### K 的选择很重要

- **K 太小（如 1）**：
  - 对噪声敏感，容易过拟合。
- **K 太大**：
  - 模型会变得太“模糊”，容易欠拟合。
- 通常通过交叉验证（cross-validation）选出最合适的 K。



##### 距离度量方式（常见）

1. **欧几里得距离（Euclidean Distance）**：

   $d(x,y)=∑i=1n(xi−yi)2d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$

2. **曼哈顿距离（Manhattan Distance）**：

   $d(x,y)=∑i=1n∣xi−yi∣d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$

3. **闵可夫斯基距离（Minkowski Distance）**（是前两者的推广）



##### KNN 的优缺点

| 优点                   | 缺点                               |
| ---------------------- | ---------------------------------- |
| 实现简单               | 预测时计算成本高（每次都得算距离） |
| 不需要训练过程         | 对高维数据表现差（维度灾难）       |
| 可用于分类也可用于回归 | 对异常值和噪声敏感                 |
| 效果直观、易理解       | 特征缩放对结果影响大（需标准化）   |



##### 注意事项

- 数据预处理非常重要：
  - 特征标准化（如 z-score、min-max scaling）防止某些特征对距离影响过大
- 通常结合 `cross-validation` 来确定 `K` 值
- 可通过 KD 树（K-D tree）或 Ball 树优化高维空间的搜索效率





#### 2.1 回归（Regression）

**核心思想：**

- 用于预测**连续数值型变量**（而非类别），目标是学习一个函数从输入 `x` 映射到输出 `y`。

**常见形式：**

###### 线性回归（Linear Regression）：

- 假设关系是线性的：

  $y=mx+c$

- 目标是最小化误差（如均方误差 MSE）来拟合参数 `m` 和 `c`

###### 多元线性回归（Multiple Linear Regression）：

- 输入变量 `x` 有多个维度：

  $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n$

###### 逻辑回归（Logistic Regression）：

- 实际上是分类算法，用于输出属于某一类的概率

  $P(y=1 | x) = \frac{1}{1 + e^{-(wx + b)}}$



#### 2.2 补充MSE(Mean Squared Error 均方误差)

MSE 是一种衡量“预测值和真实值之间误差”的方法，计算公式是：

$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

- $y_i$ 是真实值
- $\hat{y}_i $是模型预测值
- 差值平方后再求平均，可以衡量整体拟合效果好不好



#### 2.3 $R²$(R-squared）

R²（R-squared）是回归分析中非常重要的一个评估指标，被称为**判定系数**。



**一句话解释就是 R² 衡量的是模型解释目标变量变异性的能力**，也就是：

➤ **模型解释了多少比例的 y 的“变化”？**



R² 的经典定义是：

$R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}$ 

$$R^2(y, \hat{y})) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

其中：

| 符号                | 含义                                                         |
| ------------------- | ------------------------------------------------------------ |
| ${SS}_{\text{res}}$ | 残差平方和（Sum of Squares of Residuals） 也就是预测值和真实值之间的误差平方和 |
| ${SS}_{\text{tot}}$ | 总平方和（Total Sum of Squares） 也就是真实值和平均值之间的误差平方和 |
| $y_i$               | 第 i 个真实值                                                |
| $\hat{y}_i$         | 第 i 个预测值                                                |
| $\bar{y}$           | 所有真实值的平均值（目标均值）                               |

1. **SS_res** 衡量的是：

   > 我现在用模型预测，误差有多大？

2. **SS_tot** 衡量的是：

   > 如果我什么都不预测，只是拿 “平均值” 当预测，误差有多大？

3. 所以 R² 本质上是：

   $R^2 = 1 - \frac{\text{你现在的模型的误差}}{\text{啥都不干直接用平均值的误差}}$



| R² 值      | 意义                                         |
| ---------- | -------------------------------------------- |
| R² = 1     | 模型完美预测，误差为 0                       |
| R² = 0     | 模型不如直接用均值预测                       |
| R² < 0     | 模型比“用平均值”还差！😬                      |
| 0 < R² < 1 | 模型解释了一定比例的 y 的变化，越接近 1 越好 |



#### 2.4 交叉验证（Cross-Validation, 简称 CV）

**交叉验证（Cross-Validation, 简称 CV）** 是模型训练中最常用、最可靠的评估技术之一，掌握它能让你训练出更**稳健**、更**泛化**的模型.



**问:  什么是交叉验证（Cross-Validation）？**

交叉验证是一种评估模型泛化能力的方法，核心思想是：把数据切分成多个“训练集+验证集”，多次训练和测试，最后平均结果



**问: 为什么不用一次简单的 train/test 划分就好？**

一次划分太“偶然”了，可能刚好：

- 测试集全是容易预测的 → 你以为模型很好，其实没学到本质
- 测试集难度太大 → 你以为模型很差，其实换个数据就好了

所以我们不只分一次，而是 **多次分，交替使用每一部分数据来训练和验证**，最后取平均，更公平、更稳定。



**最常用的交叉验证方法：K折交叉验证（K-Fold Cross-Validation）**

**问: 如何做?**

假设你设置 K=5，整个流程是这样的：

1. 把数据分成 5 个“相等大小的子集”
2. **每次轮流用其中一个作为验证集**，其他 4 个用作训练集
3. 共训练和评估 5 次
4. **对这 5 次的得分（如 MSE、R²、准确率）取平均** → 最终评分



**cross_val_score 常见参数**

| 参数        | 作用                                                         |
| ----------- | ------------------------------------------------------------ |
| `estimator` | 要使用的模型，比如 `LinearRegression()`                      |
| `X`、`y`    | 数据                                                         |
| `cv=5`      | 使用几折交叉验证                                             |
| `scoring`   | 使用什么指标评估，常见：`'r2'`、`'neg_mean_squared_error'`、`'accuracy'` |



**总结 : 交叉验证 = 多次训练+验证 → 取平均 → 更稳健的评估方法**



#### 3. 朴素贝叶斯分类器（Naive Bayes）

##### 核心思想：

- 基于**贝叶斯定理（Bayes' Theorem）\**构建的分类器，假设各个特征之间\**条件独立（conditional independence）**

  $P(y∣x1,x2,...,xn)∝P(y)∏i=1nP(xi∣y)P(y|x_1, x_2, ..., x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)$

##### 举例说明：

- 假设你想预测一个邮件是否为垃圾邮件（spam），你会根据是否包含某些词（如“free”、“win”）来估算其概率。
- 如果训练集中出现“free”的邮件有 90% 是垃圾邮件，则 `P(spam|free)` 会很高。

##### 优点：

- 速度快、占用内存小、对高维数据效果好（如文本分类）
- **尤其适合文本数据**（如情感分析、垃圾邮件识别）

##### 缺点：

- 强假设“特征之间独立”在现实中通常不成立

------

#### 4️⃣ 简单神经网络（Simple Neural Networks）

##### 📌 核心思想：

- 模拟人脑神经元的结构，通过网络层级连接输入与输出，学习复杂的非线性映射关系。

##### 🧱 基本结构：

- **输入层（Input Layer）**：特征向量
- **隐藏层（Hidden Layers）**：由多个神经元（节点）组成，进行加权和 + 激活函数（如 ReLU、sigmoid）
- **输出层（Output Layer）**：分类概率或数值预测

##### ⚙️ 单个神经元的计算：

$z=∑wixi+b,激活后a=σ(z)z = \sum w_i x_i + b,\quad \text{激活后} \quad a = \sigma(z)$

其中 $σ\sigma$ 是激活函数（如 sigmoid, ReLU）

##### 🧠 训练过程：

- 使用反向传播（backpropagation）和梯度下降（gradient descent）不断优化网络中的权重 `w` 和偏置 `b`

##### ✅ 优点：

- 可逼近任意复杂函数（Universal Approximation Theorem）
- 支持多分类、多任务、图像处理、语言建模等多种应用

##### ❌ 缺点：

- 可解释性差，需要大量数据训练
- 易过拟合，尤其在参数过多时